### 1.Transformer为何使用多头注意力机制？（为什么不使用一个头）
- 因为不同的头可以捕捉不同的特征，这样使得模型能够更加全面的理解和处理输入序列。
- 多个头的每个头都是相互独立的，这意味着GPU可以并行计算每个头，充分利用硬件的并行计算能力。
- 不使用多头，假设序列长度为n，维度为d，一个头对应的注意力矩阵的计算复杂度为O(L²*d),即每个元素都要经过d次乘法，一共L²个元素。使用多头，假设一共x个头，每个头h个维度，则每个头的计算复杂度为：O(L²h)，计算量大大减少。
### 2.Transformer为什么Q和K使用不同的权重矩阵生成，为何不能使用同一个值进行自身的点乘？
- 因为key和query分别代表了每个词的键值和查询，再利用向量点乘计算相似度从而去更新value。如果用同一个权重矩阵去生成，那么每个token对应的key和value就会很相似，无法提取token的不同的语义信息。
### 3.Transformer计算attention的时候为何选择点乘而不是加法？两者计算复杂度和效果上有什么区别？
- 因为点积有相应的数学含义，即计算两个向量的相似度，这符合我们设计的key，query对的作用。而且点积并行起来就是矩阵乘法，GPU对于矩阵乘法有很好的优化。
### 4.为什么在进行softmax之前需要对attention进行scaled（为什么除以dk的平方根），并使用公式推导进行讲解
- 因为如果假设q和k中的每一个值都是一个符合标准正态分布的随机变量的话，则q和k的转置点积之后得到的随机变量也符合正态分布，但均值为0方差为d,d是k向量的第二个维度。则我们再除以根号d也就是标准差，符合标准正态分布。则不论d的大小是多少，得到的点积结果都符合标准正态分布。点积结果的分布不因d的大小而改变。
### 5.在计算attention score的时候如何对padding做mask操作？
- 我们在处理不同长度的序列时，需要利用padding来填充较短的序列。而填充的序列在attention_mask中对应的值为0.那么在计算注意力分数的时候，我们会将padding的位置的点积设置为负无穷大，这样在softmax操作之后，注意力分数就会变为0，不会影响最终的加权输出。
### 6. 为什么在进行多头注意力的时候需要对每个head进行降维？
- 减少计算量，同问题1
### 7.大概讲一下Transformer的Encoder模块？
- 解码器模块主要有add&norm层，FFN层还有multi-head attn层组成。其中norm层使用layernorm， add为残差连接，FFN为一个前馈神经网络：linear层+ReLU+Linear层构成。
### 8.简单介绍一下Transformer的位置编码？有什么意义和优缺点？
- 绝对位置编码使用正余弦位置编码。位置编码矩阵大小和输入大小相同，即为N*D，其中N是序列长度，D是维度。位置编码矩阵的每一行的值都是由正弦函数和余弦函数交替计算得出。通过这种方式，不同的位置对应的向量在一定程度上保持了 位置的相对关系。
### 9.你还了解哪些关于位置编码的技术，各自的优缺点是什么？
### 10.简单讲一下Transformer中的残差结构以及意义。
- 残差结构的出现可以使得梯度在回传的过程中有了另一条可供选择的路径，防止了梯度消失和梯度爆炸的可能。
### 11.为什么transformer块使用LayerNorm而不是BatchNorm？LayerNorm 在Transformer的位置是哪里？
- 因为batchnorm是在特征维度，即batch维度做归一化，对于序列长度不同的batch数据，batch计算的某些维度的均值和方差不具有代表性。而且每一个batch数据的同一个位置的token也并不属于同一个维度。而layernorm是在token维度进行归一化，即对一个token的向量表示进行归一化。在原始transformer中，norm层是在attention层之后为postnorm。后续人们将其放在了attention之前。
### 12.