# FlashAttention
## 简介
- FlashAttention 是一种高效实现的注意力机制，特别是用于大规模 Transformer 模型（如 GPT 和 BERT）的加速计算。传统的注意力计算（如自注意力机制）在处理长序列时会遇到性能瓶颈，因为其时间复杂度是 O(N²)，N 是序列的长度。FlashAttention 的目标是通过优化计算方法和减少内存访问，显著提高这一过程的效率，尤其是在 GPU 上。
- FlashAttention 主要用于加速 自注意力机制（Self-Attention），它是在 Transformer 模型中计算注意力的核心部分。FlashAttention 的一个关键优势是，它能减少内存带宽的需求，并利用硬件特性（如 CUDA 和特定的 GPU 架构）来优化计算速度。

## 动机
传统的注意力机制作用于长序列机制的时候，注意力计算的时间复杂度和空间复杂度很高。  
- **时间复杂度 O(N²)**：对于每一对输入 token 之间都要计算注意力，这对于较长的序列而言非常低效。  
- **内存开销**：传统的注意力实现需要存储大量的中间计算结果，特别是在深度学习模型训练时，内存的消耗会非常大。  
所以flashatten在以下几个方面进行优化：  
- **减少内存访问**：FlashAttention 减少了对不必要的内存区域的访问，并且通过一些算法优化将内存带宽利用到最大化，减少了模型训练中的瓶颈。  
- **高效计算**：在GPU上优化了矩阵计算，充分利用了GPU中的矩阵乘法加速硬件(Tensor Cores)  
- **利用局部性**：FlashAttention 将长序列分割成较小的块，通过优化计算过程的局部性，减少了冗余计算，提高了缓存效率  
## 总结
**利用硬件加速和内存优化，解决传统注意力机制计算带来的内存占用和时间复杂度过高的问题**
